# August 17

* compare variance/se at a freq (from M=100)
- YOU'll need to put the c estimate INSIDE the loop, or use apply to the FULL dataset
* ch 4: investigating how to make the block size adaptive: if B is a fn of time.
    * small B in center for instance, then large B at edges?
* compare to Azadeh
- CBC for stationary -> column average -> compare to multitaper
- see how this works for 1 simulation
* look up how to only get eigenvec corresponding 1st eigenvalue or largest eigenvalue 
- estimate c, get residual estimate of S_Y, outer product should be smooth now
* do a gump

* after propsp, look up LINPACK and how to use different eigens 

#### Skye's Wishlist

* fix code for large lattice of g est plots
* renaming convention involving "rev" factor
* get data for B=400 NOT rev

#### Results so far

* Got g estimates directly from sims (not mean of sims)
* Plots for new g estimates
    - BC looks a bit better than CBC, now :(
* smoothed BC and CBC by taking outer product of g est and colmeans
    - this works for UMPs!!
    - Honestly CBC looks a bit better than BC :) for M=100
    - for M=1... Honestly BC is better whatever but it WORKS
    
#### Things to bring up in next meeting

* standard error of $\tilde g$ is very small

---

# August 24

* Alternative estimate of Y(t): x(t)/c(t) (don't use TFS, that's where you got stuck w indeces)
    * this est might be better for detecting line components present in Y(t)
* iteration: get g-hat from SBC and SCBC using same method to get the g which made those matrices
* compare g to Azadeh's estimate (skye... do you think the panel might ask you if you've done this? Make sure you at least take a look)
* compare TFS colMeans to multitaper of Y. Better in different contexts?

**LITERATURE:** Senay 2018. Figure out BSS (see my current understanding of Glen's summary, below).

* similar to classical stats' "independent component analysis"
* suppose you have two observed series, and each is a lin combo of two subseries/signals
* cross spectra should have 0's off diagonal, assuming the signals are uncorrelated
* Senay diagonalizes the matrix $\begin{bmatrix}S_{xx} & S_{xy} \\ S_{yx} & S_{yy}\end{bmatrix}$ at each $(t,f)$ pair
* corresponding SVD gives us weights (coefs) distinguishing the signals. weights assumed constant over time... what if they changed?
    * two people talking on their own hot mics in the same room, mics picking up signals from both but at different amps
    * what if the people moved around so those amps changed - p1 steps away from mic towards p2, etc. 
    * brainwaves probably do this to electrodes

**PAPER SUBMISSION**  
ICASP: write up a paper on chapter 3 results for next week. Look at Azadeh's SSP for inspo. Cite Percy-Waldy 20' for multitaper.

#### Skye's Wishlist

* create a figure showing results for the Azadeh method of getting g
    * show negatives?
    * show U1 and V1?
- get smoothed BC/CBC for AR(2), compare to multitaper
* get y(t) from x(t)/c(t), multitaper, and compare to true psd
* go through *Plotter_1.Rmd* and switch `lines()` to `sadd()` 
* create BC/CBC for a gump

#### Results so far

* Azadeh's method works perfectly for the theoretical TFS, but somehow is negative when taken from the BC and/or CBC
* colmeans work well for mean of 100 sims, undershoot a bit I think tho
* UM... REALLY GOOD RESULTS FOR M=1 like wtf good

#### Things to bring up in next meeting

# August 31

PBC poly BC
Apply stuff from section 4 to 1BC and 2BC
that way it doesn't propose a new estimator, just considers it
introduces via the system of approxes 10 thru 12, say PBC 1 uses f10-11 first terms only

Combine sections 6 and 7 into a single section: conc and discussions
rename "stationary foundation" to spectrum
combine sections 5.2 and 5.3 (both about UMPs), eliminate section 5.1

make sure to include azadeh's estimator, just multiply it by -1, use as a "baseline"

try smooth yf for stationary with line components
for proposal, yf might have robustness benefits
careful to contextualize yf: better for stationary with naturally smooth spectrum (might not work if line component is present)
    possible application: remove line components detected by harmonic f, then use yf to get spectrum for residuals
    
in ump case, don't worry about baseline for yf

why are we doing the eigenval problem
    hat Sy changes with respect to time due to noise and time boundary issue stuff
    taking ratios takes into account all those specs, the cancel like unit conversion style
    the ratios of the hat sy's form a distribution centered at 1
    ** okay maybe this will improve the yf estimate from colsums, do it too
      do the code
      
only do image plots for cbc bro
to get se: get yf for each simulation and get standard error from THAT (you'll need trial object)

#### Results so far

* BC and CBC do not pick up line components at all :(
* got sd for yf estimates (old verion, not eigen problem), it's fine
* got yf estimate from eigenproblem (similarly to g est), it's way off by scale factor, even for stationary :( 
* actually got azadeh version of stuff, actually did my stuff too like i have results now for real

#### Skye's Wishlist

- Make a plot comparing gest to azadeh method
- get plots for new results (except line component stuff fuck that)
- create the image plots that you'll actually use in paper

#### The Plan

1. get and plot Azadeh Estimates of g, compare to mine [DONE]
2. get and plot Azadeh Estimates of syf, compare to mine (NEW version) [DONE]
3. finalize image plots for paper
    * CBC versus true TFS
    * smoothed CBC

#### Discussion

1. I'm normalizing Azadeh's estimates so that they max out at 1. If I want the standard error, do I get that from the normalized versions? Does that defeat the purpose? 
    * I'm normalizing the mean of the estimates, not averaging the normalized estimates
2. Similarly, I should take the average of the smoothed sgrams and not smooth the average, yes?

# September 5th

#### To do (paper)
- change SY(f) to $S_Y(f)$ in 2d plots
* check results for B=50 using new code, make sure something's not wonky
* take out stuff about $\hat y(t) = x(t)/\hat c(t)$
- all titles "smoothed" not "smooth"
- make 2x1, 3x1 columns for plots in fig1 and fig2/3, respectively
* take out eq tags for eqs not referenced in the doc

#### To consider
* mean smoothed is more symmetric and pinpoints location of maximum power better than raw
* call em smoothed g and smoothed sy


#### for proposal maybe
* put $\hat y(t) = x(t)/\hat c(t)$ on the $S_Y(f)$ plot

In general, I'm normalizing the average result over 100 *un-normalized* estimates. If I do it the other way around (ie: taking the average over 100 *normalized* estimates) then the final result is guaranteed to underestimate the true function. How do I calculate the standard error according to this method? I don't think it's correct to simply take the average standard error across all estimates and then normalize it in the same way. When I say "normalize," I mean dividing a function (example: $\hat g$) by its maximum value, so that the normalized range maxes out at 1.

* instead, normalize by t=500

# September 12th

#### Skye's Agenda

* I've updated all the code, gotten nice results/plots, backed up
    - New data offsets c(t) so that it isn't symmetric about t=500. This leaves different curvature in the time-boundary regions, which is especially distinct if we modulate by 1-c(t)
    - i also have some basic results for a GUMP-2
* Today my goal is to get a full draft of chapter 3 done
* Then I'm going to go through and update chapter 2 with the literature used throughout chapter 3
* Wednesday through Friday I'm going to aim to finish chapter 4 and update chapter 2 accordingly
* Going to an engagement party this weekend, but only for the weekend like I'll work Friday and we leave at 4pm, then I'm back Monday morning

# October 6th
* stuff for intro to 4.2
    - Read papers Glen put in the slack
    - Do a connected papers on Senay
* you need to show comprehension (breadth)

* (26) is like a time-dep eigencoef
* why is senay
    - uses tapers to reduce leakage

* Talk about wavelets
    - don't explain them just say that they exist, I'm aware of them
    - "here's a bunch of papers that do this: blablabla"
    
* Data analysis
    - what data is relevant
    - Seismic for UMPS (put this first)
    - list some papers that have used real data
    - how do I obtain real data? Say you don't know

* bundle coherence into BSS section
    - Extend our estimates of the evolutionary spectrum to the cross case
    - repeat "definition was proposed in melard and schutter ..." from senay, part before eq (5)
    













